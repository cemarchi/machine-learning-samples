{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3",
   "language": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "# Random Forest\n",
    "<p>Pertence a categoria de algoritmos <i>ensemble</i> que combinam vários modelos de aprendizado de máquina para criar\n",
    "modelos mais poderosos. Esse modelo é eficaz para uma ampla gama de conjuntos de dados seja para classificação quanto para regressão, criando aleatóriamente várias árvores de decisão onde cada uma é encarregada de realizar uma predição.</p>\n",
    "<p>A predição final é escolhida pelo método de votação por maioria. Por exemplo, no caso para classificação, cada classificador criado vota para uma classe dentre todas as classes de destino. A classe alvo que tiver maior número de votos é considerada como a prevista.</p>\n",
    "\n",
    "## Vantagens\n",
    "\n",
    "<ul>\n",
    "<li>Fácil de interpretar</li>\n",
    "<li>Pouco risco de <i>overfitting</i></li>\n",
    "<li>Pode usar ambos dados categóricos e contínuos</li>\n",
    "<li>Pode utilizar grandes conjuntos de dados e com alta dimensionalidade</li>\n",
    "<li>Não é sensível a <i>outliers</i></li>\n",
    "<li>Não paramétrico</li>\n",
    "<li>Ótima predição frente a outros algoritmos de aprendizado de máquina</li>\n",
    "<li>Não requer normalização dos dados</li>\n",
    "<li>Consegue lidar com valores faltantes no conjunto de dados</li>\n",
    "<li>Consegue lidar com conjunto de dados desbalanciados</li>\n",
    "</ul>\n",
    "## Desvantagens\n",
    "\n",
    "<ul>\n",
    "<li>Lento para realizar predição</li>\n",
    "<li>Necessário poder computacional e recursos</li>\n",
    "<li>Muitos parâmetros de configuração</li>\n",
    "<li>Estado</li>\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## Exemplo\n",
    "<p>Vejamos um exemplo para criar modelo preditivo utilizando o algoritmo <i>Random Forest</i> para classificação de flores de íris.</p>\n",
    "\n",
    "### Dataset: UCI Iris Dataset \n",
    "\n",
    "<p>Esse conjunto de dados de íris contém quatro variáveis (<i>features</i>) ​​que medem várias partes das flores da íris de três espécies relacionadas (<i>target</i>). A utilização desse conjunto de dados é muito comum pela comunidade de aprendizado de máquina, pois os dados exigem muito pouco pré-processamento, sendo ideal para o nosso exemplo onde o foco é na utilização do algoritmo.</p>\n",
    "\n",
    "<p>Caso tenha interesse em conhecer mais sobre esse conjunto de dado, por favor vide <a href=\"https://scikit-learn.org/stable/datasets/toy_dataset.html#iris-dataset\">aqui</a>.</p>\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "### Importação dos pacotes"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import RandomizedSearchCV"
   ]
  },
  {
   "source": [
    "### Carregamento do conjunto de dados"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = load_iris( return_X_y = True)"
   ]
  },
  {
   "source": [
    "### Configuração dos parâmetros do algoritmo\n",
    "<p><i>Random forest</i> contém vários parâmetros configuráveis e isso aumenta a complexidade de utilização.</p> Vejamos a seguir uma breve descrição dos parâmetros mais utilizados e também os valores mais adequados de utilização.\n",
    "\n",
    "<ul>\n",
    "<li><b>n_estimators</b>: quantidade de árvores de decisão para predição. Se o seu conjunto de dados for complexo pode exigir maior valor para esse parâmetro, além disso, quanto maior a quantidade de estimadores exigirá mais recursos e poder computacional.</li>\n",
    "<li><b>criterion</b>: me a qualidade de divisão do nó na árvore, podendo ser o índice de Gini (impureza Gini <a href=\"https://towardsdatascience.com/what-is-gini-impurity-how-is-it-used-to-construct-decision-trees-75d01ed78812\">mais detalhes</a>) e Entropia (ganho de informação <a href=\"https://towardsdatascience.com/entropy-how-decision-trees-make-decisions-2946b9c18c8\">mais detalhes</a>).</li>\n",
    "<li><b>bootstrap</b>: Se as amostras de <i>bootstrap</i> são usadas para construir árvores. Se for negativo, todo o conjunto de dados é usado para construir cada árvore do contrário, os dados são escolhidos estatisticamente levando a novas amostras.</li>\n",
    "<li><b>max_depth</b>: número máximo de níveis para cada árvore.</li>\n",
    "<li><b>max_features</b>: número máximo de características levadas em consideração para dividir o nó. Podendo ser valor da quantidade de características ou a fração, a raíz quadrada do número de características, logarítmo de base 2 do número de características. É interessante testar com <i>sqrt</i>, <i>log2</i>, <i>none</i> e fração entre 0 e 1 de 100 espaçados igualmente.</li>\n",
    "<li><b>min_samples_leaf</b>: número mínimo de amostras na folha da árvore. Esse valor tende a ser pequeno.</li>\n",
    "<li><b>min_samples_split</b>: número mínimo de amostras para dividir o nó, sendo melhor utilizar valores entre 0.05 (5%) a 1.0 (100%) da amostra de 10 espaços iguais.</li>\n",
    "</ul>\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_depth = [int(x) for x in np.linspace(2, 100, num = 80)]\n",
    "max_depth.append(None)\n",
    "        \n",
    "max_features = [x for x in np.linspace(0.1, 1, num = 80)]\n",
    "max_features.extend(['sqrt', None, 'log2'])\n",
    "\n",
    "params = {\n",
    "    'n_estimators': range(100, 500, 10),\n",
    "    'criterion': ['gini', 'entropy'],\n",
    "    'bootstrap': [True, False],\n",
    "    'max_depth': max_depth,\n",
    "    'max_features': max_features,\n",
    "    'min_samples_leaf': range(1, 10, 1),\n",
    "    'min_samples_split': np.linspace(0.05, 1.0, 10, endpoint=True)}"
   ]
  },
  {
   "source": [
    "### Inicia o treinamento da inteligência com espaço de busca"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "RandomizedSearchCV(cv=3, estimator=RandomForestClassifier(), n_iter=500,\n",
       "                   n_jobs=-1,\n",
       "                   param_distributions={'bootstrap': [True, False],\n",
       "                                        'criterion': ['gini', 'entropy'],\n",
       "                                        'max_depth': [2, 3, 4, 5, 6, 8, 9, 10,\n",
       "                                                      11, 13, 14, 15, 16, 18,\n",
       "                                                      19, 20, 21, 23, 24, 25,\n",
       "                                                      26, 28, 29, 30, 31, 33,\n",
       "                                                      34, 35, 36, 37, ...],\n",
       "                                        'max_features': [0.1,\n",
       "                                                         0.11139240506329115,\n",
       "                                                         0.12278481012658228,\n",
       "                                                         0.1341772151898734,\n",
       "                                                         0.14556962025...\n",
       "                                                         0.36202531645569624,\n",
       "                                                         0.37341772151898733,\n",
       "                                                         0.38481012658227853,\n",
       "                                                         0.3962025316455696,\n",
       "                                                         0.4075949367088607,\n",
       "                                                         0.4189873417721519,\n",
       "                                                         0.430379746835443, ...],\n",
       "                                        'min_samples_leaf': range(1, 10),\n",
       "                                        'min_samples_split': array([0.05      , 0.15555556, 0.26111111, 0.36666667, 0.47222222,\n",
       "       0.57777778, 0.68333333, 0.78888889, 0.89444444, 1.        ]),\n",
       "                                        'n_estimators': range(100, 500, 10)},\n",
       "                   scoring='f1_macro', verbose=False)"
      ]
     },
     "metadata": {},
     "execution_count": 5
    }
   ],
   "source": [
    "rfc_search = RandomizedSearchCV(RandomForestClassifier(), \n",
    "                                param_distributions=params, \n",
    "                                n_iter=500,\n",
    "                                cv=3,\n",
    "                                scoring = 'f1_macro',\n",
    "                                verbose=False,\n",
    "                                n_jobs=-1)\n",
    "\n",
    "rfc_search.fit(X, y)"
   ]
  },
  {
   "source": [
    "### Resultados\n",
    "<p>Os resultados apresentados aqui dão uma ideia da qualidade do modelo gerado. Vale ressaltar que o propósito deste exemplo é no entendimento e configuração do algoritmo.</p>"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Best params:\n{'n_estimators': 220, 'min_samples_split': 0.05, 'min_samples_leaf': 1, 'max_features': 0.7949367088607595, 'max_depth': 36, 'criterion': 'gini', 'bootstrap': True}\nBest score: 0.9664230631972569\nClassifier: RandomForestClassifier(max_depth=36, max_features=0.7949367088607595,\n                       min_samples_split=0.05, n_estimators=220)\n"
     ]
    }
   ],
   "source": [
    "classifier_rf = rfc_search.best_estimator_ \n",
    "print(f'Best params:\\n{rfc_search.best_params_}')\n",
    "print(f'Best score: {rfc_search.best_score_}')\n",
    "print(f'Classifier: {classifier_rf}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}